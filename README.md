## Relat√≥rio: Regress√£o Log√≠stica Regularizada
## Atividade 2: da materia de Intelig√™ncia Artificial Conexionista (Mestrado UFSC)

### 1. Objetivo
Este relat√≥rio descreve a implementa√ß√£o de um modelo de regress√£o log√≠stica regularizada para classifica√ß√£o bin√°ria, aplicado a dois conjuntos de dados:

- **Previs√£o de admiss√£o universit√°ria** com base em notas de exames.
- **Classifica√ß√£o de microchips** em "aprovados" ou "rejeitados" em testes de qualidade.

### 2. Principais Etapas Implementadas

#### a) Fun√ß√µes Essenciais

- **Sigmoid:** Fun√ß√£o para mapear valores cont√≠nuos em probabilidades (entre 0 e 1).

```python
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

- **Custo e Gradiente Regularizado:** C√°lculo da fun√ß√£o de custo \( J(\theta) \) e gradiente para regress√£o log√≠stica com regulariza√ß√£o L2.

```python
def costFunctionReg(theta, X, y, lambda_):
    h = sigmoid(X.dot(theta))
    termo_regularizacao = (lambda_ / (2 * len(y))) * np.sum(theta[1:]**2)
    J = (-1/len(y)) * (y @ np.log(h) + (1 - y) @ np.log(1 - h)) + termo_regularizacao
    grad = (1/len(y)) * X.T @ (h - y) + (lambda_ / len(y)) * np.concatenate([[0], theta[1:]])
    return J, grad
```

- **Previs√£o:** Classifica√ß√£o bin√°ria (0 ou 1) com base em probabilidades.

```python
def predict(theta, X):
    return (sigmoid(X.dot(theta)) >= 0.5).astype(int)
```

#### b) Mapeamento de Features Polinomiais

Transforma√ß√£o de 2 features originais em 28 termos polinomiais (grau 6). Exemplo de feature gerada: \( x_1^2, x_1 x_2^3, x_2^6 \).

```python
def mapFeature(X1, X2, degree=6):
    out = [np.ones(X1.size)]
    for i in range(1, degree+1):
        for j in range(i+1):
            out.append((X1 ** (i - j)) * (X2 ** j))
    return np.column_stack(out)
```

#### c) Otimiza√ß√£o com `scipy.optimize`

Uso da fun√ß√£o `minimize` para encontrar os par√¢metros \( \theta \) que minimizam o custo.

```python
result = optimize.minimize(
    fun=costFunctionReg,
    x0=initial_theta,
    args=(X, y, lambda_),
    method='TNC',
    jac=True,
    options={'maxiter': 100}
)
```

### 3. Visualiza√ß√£o e An√°lise

#### a) Gr√°ficos Recomendados

- **Dados de Treinamento:**
    - Pontos marcando exemplos positivos \( y = 1 \) e negativos \( y = 0 \).
    - Sugest√£o de legenda:
        - `k*` para aprovados.
        - `ko` (amarelo com borda preta) para rejeitados.

- **Fronteiras de Decis√£o para Diferentes \( \lambda \):**
    - \( \lambda = 0 \): Fronteira complexa (overfitting).
    - \( \lambda = 1 \): Fronteira balanceada.
    - \( \lambda = 100 \): Fronteira simplista (underfitting).

#### b) Imagens Geradas

##### **Fronteira de Decis√£o para Diferentes Valores de \( \lambda \)**

**\( \lambda = 100 \)**
![Lambda 100](img/Lambda100.png)

**\( \lambda = 10 \)**
![Lambda 10](img/Lambda10.png)

**\( \lambda = 1 \)**
![Lambda 1](img/Lambda1.png)

**\( \lambda = 0 \)**
![Lambda 0](img/Lambda0.png)

**Distribui√ß√£o dos Dados**
![Distribui√ß√£o](img/distribuicao.png)

**Classifica√ß√£o de Admiss√£o Universit√°ria**
![Admiss√£o Universit√°ria](img/admissao.png)

### 4. Impacto da Regulariza√ß√£o

| \( \lambda \) | Comportamento        | Acur√°cia no Treino |
|-------------|--------------------|-----------------|
| 0           | Overfitting         | ~100%          |
| 1           | Generaliza√ß√£o ideal | ~83-89%        |
| 100         | Underfitting        | ~50-60%        |

### 5. Conclus√£o

A regulariza√ß√£o \( \lambda \) controla a complexidade do modelo:

- **\( \lambda \) pequeno:** Modelo flex√≠vel, propenso a overfitting.
- **\( \lambda \) adequado:** Equil√≠brio entre vi√©s e vari√¢ncia.
- **\( \lambda \) grande:** Modelo r√≠gido, subajustado.

A transforma√ß√£o polinomial permite capturar rela√ß√µes n√£o lineares, mas exige cuidado para evitar overfitting.

---

Caso tenha d√∫vidas ou precise de melhorias, sinta-se √† vontade para modificar o c√≥digo e as an√°lises! üöÄ
